# Create a Data Warehouse with AWS Redshift

--------------------------------------------

### Introduction

A music streaming startup, Sparkify, has grown their user base and song database and want to move their
processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity
on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, we are tasked with building an ETL pipeline that extracts their data from S3,
stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team
to continue finding insights into what songs their users are listening to.

In this project, we will need to load data from S3 to staging tables on Redshift and execute SQL
statements that create the analytics tables from these staging tables.

![data](https://video.udacity-data.com/topher/2022/May/62770f73_sparkify-s3-to-redshift-etl/sparkify-s3-to-redshift-etl.png)

We'll be working with 3 datasets that reside in S3. Here are the S3 links for each:

* Song data: s3://udacity-dend/song_data
* Log data: s3://udacity-dend/log_data
* This third file s3://udacity-dend/log_json_path.json - contains the meta information that is required by AWS to correctly load s3://udacity-dend/log_data

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format
and contains metadata about a song and the artist of that song. The files are partitioned by the first
three letters of each song's track ID. For example, here are file paths to two files in this dataset.
* song_data/A/B/C/TRABCEI128F424C983.json
* song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like:
~~~~
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
~~~~
The second dataset consists of log files in JSON format generated by this event simulator based on the
songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based
on configuration settings.

The log files in the dataset we'll be working with are partitioned by year and month. 
For example, here are file paths to two files in this dataset.
* log_data/2018/11/2018-11-12-events.json
* log_data/2018/11/2018-11-13-events.json

The data look like this:
![data](https://video.udacity-data.com/topher/2019/February/5c6c3ce5_log-data/log-data.png)
--------------------------------------------

### ETL

The ETL process can be described as follows:

1. First, we create staging tables on Redshift
2. Then, we copy the data from public S3 Buckets into the staging tables on Redshift
3. Then, we create the Star Schema on Redshift

---------------------------------------------
### Steps
1. After creating your user on the AWS console, open the dwh.cfg file and enter your KEY and SECRET
2. Run the "create_and_subsequently_delete_cluster" Jupyter Notebook to create the cluster and the associated permissions
DO NOT DELETE THE CLUSTER YET
3. Go back to the dwh.cfg file and fill out the information obtained at the bottom of the 
create_and_subsequently_delete_cluster file
4. Run the 'etl.py' file in the Terminal, which will execute the functions from the create_tables.py and the 
queries from sql_queries.py files
5. Run the cells in the "create_and_subsequently_delete_cluster" Jupyter Notebook to delete the cluster and the 
associated permissions